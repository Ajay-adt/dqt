{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DQT Framework","text":"<p>This notebook template automates the process of generating mkdocs index.md files based on the results of your Databricks data quality (DQ) checks. It leverages Great Expectations to define and run expectations against your data, and then creates a report summarizing the results.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>python setup.py sdist bdist_wheel</code> - Create wheel file.</li> <li><code>pip install dist/your_package-1.0.0.tar.gz</code> - Install from source distribution.</li> <li> <p><code>pip install dist/your_package-1.0.0-py3-none-any.whl</code> -  Install from wheel distribution.</p> </li> <li> <p><code>mkdocs new [dir-name]</code> - Create a new project.</p> </li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>\u251c\u2500\u2500 lib\n\u2502   \u251c\u2500\u2500 docs\n\u2502   \u2502   \u251c\u2500\u2500 dependency.md        # Documentation about project dependencies\n\u2502   \u2502   \u251c\u2500\u2500 img                  # Image resources for documentation\n\u2502   \u2502   \u251c\u2500\u2500 index.md             # Main documentation index\n\u2502   \u2502   \u251c\u2500\u2500 javascripts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mathjax.js       # JavaScript library for math typesetting\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 tawk.js          # JavaScript for Tawk.to integration\n\u2502   \u2502   \u2514\u2500\u2500 quickstart.md        # Quickstart guide for the project\n\u2502   \u251c\u2500\u2500 dqt\n\u2502   \u2502   \u251c\u2500\u2500 DQAction.py          # Module for data quality actions\n\u2502   \u2502   \u251c\u2500\u2500 DQEngine.py          # Module for data quality engine\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py          # Initialization file for the dqt package\n\u2502   \u2502   \u251c\u2500\u2500 cli.py               # Command-line interface module\n\u2502   \u2502   \u251c\u2500\u2500 exception.py         # Module for custom exceptions\n\u2502   \u2502   \u251c\u2500\u2500 logger.py            # Module for project logging\n\u2502   \u2502   \u2514\u2500\u2500 utils.py             # Utility module\n\u2502   \u251c\u2500\u2500 mkdocs.yml               # Configuration file for MkDocs documentation\n\u2502   \u251c\u2500\u2500 requirements_dev.txt     # Development dependencies\n\u2502   \u2514\u2500\u2500 setup.py                 # Setup script for the project\n\u2514\u2500\u2500 readme.md                    # Project readme file\n</code></pre>"},{"location":"#key-functionalities","title":"Key functionalities:","text":"<ul> <li>Parses DQ output: Reads the output of your DQ checks (assumed to be in JSON format) and converts it to a Python object for further processing.</li> <li>Sets up Azure Blob Storage and Data Docs site: Configures the necessary resources on Azure Blob Storage to store the generated mkdocs site and integrates it with Data Docs for visualization.</li> <li>Defines custom expectations: Allows you to define custom expectations in YAML format and add them to an expectation suite for specific DQ checks.</li> <li>Builds and runs batch request: Creates a batch request based on your data source and chosen expectations, and executes the validation against your data.</li> <li>Creates checkpoint and sends notification: Creates a checkpoint to save the validation results and sends notifications based on the outcome (e.g., via Microsoft Teams).</li> <li>Logs details to database: Records information about the DQ check run and its results to a database table for further analysis.</li> </ul>"},{"location":"#input-requirements","title":"Input requirements:","text":"<ul> <li>Databricks notebook: This template is designed to run within a Databricks notebook environment.</li> <li>DQ output: The notebook expects your DQ checks to generate JSON output containing details about the validation results.</li> <li>Azure Blob Storage credentials: You need to provide access credentials for the Azure Blob Storage container where you want to store the generated mkdocs site.</li> <li>Data Docs configuration: Configure your Data Docs site with the appropriate information to connect to Azure Blob Storage.</li> <li>Custom expectations (optional): If you want to define custom expectations beyond the pre-defined ones, you need to provide a YAML file containing your expectation definitions.</li> </ul>"},{"location":"#output","title":"Output:","text":"<ul> <li>mkdocs index.md file: The notebook generates an index.md file for your Data Docs site, summarizing the results of your DQ checks.</li> <li>Checkpoint: A checkpoint is created in your Great Expectations context to store the validation results for further reference.</li> <li>Database log (optional): The notebook can optionally log details about the DQ check run and its results to a database table.</li> </ul>"},{"location":"#security-considerations","title":"Security considerations:","text":"<ul> <li>This template currently stores Azure Blob Storage and GXLOG database connection strings as variables within the notebook. It's recommended to use Secret Management Vault or Databricks Secrets for secure storage and access to these credentials.</li> </ul>"},{"location":"#limitations-and-improvements","title":"Limitations and improvements:","text":"<ul> <li>The template is currently designed for DQ outputs in JSON format. Adapting it to other output formats might require modifications.</li> <li>Some parts of the notebook could benefit from more refined error handling for robustness.</li> <li>Adding comments throughout the notebook would significantly improve its readability and understandability for future users.</li> <li>Consider making the template more modular for wider use with different DQ outputs and customization options.</li> </ul> <p>Overall, this template provides a powerful foundation for automating DQ reporting in Databricks. With further refinement and security considerations, it can be a valuable tool for ensuring data quality and transparency within your data pipelines.</p>"},{"location":"#additional-notes","title":"Additional notes:","text":"<ul> <li>This documentation is a general overview, and specific details about the notebook implementation may vary.</li> <li>Feel free to ask any questions or request further explanations about any part of the template.</li> </ul> <p>I hope this documentation provides a clearer understanding of the functionalities and purpose of the notebook template. Please let me know if you have any further questions!</p>"},{"location":"dependency/","title":"Prerequesties","text":"<ul> <li>great-expectations[azure]: 0.18.5</li> <li>python-box: 7.1.1</li> <li>ensure: 1.0.4</li> <li>PyYAML: 6.0.1</li> </ul>"},{"location":"quickstart/","title":"QuickStart","text":""},{"location":"quickstart/#import-library","title":"Import library","text":"<pre><code>    from dqt import DQEngine\n    from dqt.utils import *\n</code></pre>"},{"location":"quickstart/#create-datavalidationengine-class-object","title":"Create DataValidationEngine Class Object","text":"<pre><code>    #import \n    from dqt.DQEngine import DataValidationEngine\n\n    # Create an instance of DataValidationEngine with the provided parameters\n    engine = DataValidationEngine(\n        spark,\n        GXDS_STORAGE_ACCOUNT_NAME,\n        GXDS_CONTAINER_NAME,\n        GXDS_CONTAINER_DIR,\n        fileName,\n        fileFormat,\n        ruleList,\n        lastModifiedTimestamp,\n        pipeline_run_id\n    )\n</code></pre>"},{"location":"quickstart/#run","title":"Run","text":"<pre><code>    # Check if the data quality rule is active\n    if dqRule:\n        # If the rule is active, execute the following block\n        # This block executes when the data quality rule is active\n\n        # Call the execute_dq_rule method of the engine\n        # This method likely performs the necessary data validation and logging\n        # The results (sourceDF and dq_log) are assigned accordingly\n        sourceDF, dq_log = engine.execute_dq_rule()\n\n        # You can add additional actions within this block\n\n    else:\n        # If the rule is not active, execute the following block\n        # This block executes when the data quality rule is deactivated\n\n        # Call the execute_dq_rule_deactivated method of the engine\n        # This method likely contains logic to handle the case when the rule is deactivated\n        engine.execute_dq_rule_deactivated()\n\n        # You can add additional actions within this block\n</code></pre>"},{"location":"ruleList/","title":"Rule Type Mapping","text":"Rule ID Expectation Name Action Function Name 1 <code>expect_column_to_exist</code> (No specific action function assigned) 2 <code>expect_table_row_count_to_be_between</code> (No specific action function assigned) 3 <code>expect_table_row_count_to_equal</code> (No specific action function assigned) 4 <code>expect_column_values_to_be_unique</code> <code>remove_duplicates</code> 5 <code>expect_column_values_to_not_be_null</code> <code>handle_null_values</code> 6 <code>expect_column_values_to_be_null</code> <code>handle_null_values</code> 7 <code>expect_column_values_to_be_of_type</code> <code>handle_data_type</code> 8 <code>expect_column_values_to_be_in_type_list</code> <code>handle_data_type</code> 9 <code>expect_column_values_to_be_in_set</code> <code>handle_in_set</code> 10 <code>expect_column_values_to_not_be_in_set</code> <code>handle_not_in_set</code> 11 <code>expect_column_values_to_be_between</code> <code>handle_value_range</code> 12 <code>expect_column_values_to_be_increasing</code> <code>handle_value_trend</code> 13 <code>expect_column_values_to_be_decreasing</code> <code>handle_value_trend</code> 14 <code>expect_column_value_lengths_to_be_between</code> <code>handle_value_length</code> 15 <code>expect_column_values_to_match_regex</code> <code>handle_regex_match</code> 16 <code>expect_column_values_to_not_match_regex</code> <code>handle_regex_mismatch</code> 17 <code>expect_column_values_to_match_regex_list</code> <code>handle_regex_list_match</code> 18 <code>expect_column_values_to_match_strftime_format</code> <code>handle_strftime_format</code> 19 <code>expect_column_values_to_be_dateutil_parseable</code> <code>handle_dateutil_parseable</code> 20 <code>expect_column_values_to_be_json_parseable</code> <code>handle_json_parseable</code> 21 <code>expect_column_values_to_match_json_schema</code> <code>handle_json_schema_match</code> 22 <code>expect_column_mean_to_be_between</code> <code>handle_statistical_range</code> 23 <code>expect_column_median_to_be_between</code> <code>handle_statistical_range</code> 24 <code>expect_column_stdev_to_be_between</code> <code>handle_statistical_range</code> 25 <code>expect_column_unique_value_count_to_be_between</code> <code>handle_unique_value_count_range</code> 26 <code>expect_column_proportion_of_unique_values_to_be_between</code> <code>handle_proportion_of_unique_values_range</code> 27 <code>expect_column_most_common_value_to_be</code> <code>handle_most_common_value</code> 28 <code>expect_column_most_common_value_to_be_in_set</code> <code>handle_most_common_value_in_set</code> 29 <code>expect_column_kl_divergence_to_be_less_than</code> <code>handle_kl_divergence</code> 30 <code>expect_column_bootstrapped_ks_test_p_value_to_be_greater_than</code> <code>handle_ks_test_p_value</code> 31 <code>expect_column_chisquare_test_p_value_to_be_greater_than</code> <code>handle_chisquare_test_p_value</code>"}]}